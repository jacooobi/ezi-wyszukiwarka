{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(name):\n",
    "    with open(name) as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "        \n",
    "documents_data = read_file('documents.txt')\n",
    "keywords_data = read_file('keywords.txt')\n",
    "\n",
    "def sum_squared(values):\n",
    "    return reduce(lambda x, y: x + y, map(lambda x: x ** 2, list(values)))\n",
    "\n",
    "def tokenify(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    tokens = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create documents\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import *\n",
    "\n",
    "docs = documents_data.split('\\n\\n')\n",
    "documents = []\n",
    "\n",
    "for i, doc in enumerate(docs): \n",
    "    doc_data = doc.split('\\n')\n",
    "    title = doc_data[0]\n",
    "    body = ' '.join(doc_data[1:len(doc_data)])                        \n",
    "\n",
    "    documents.append({'idx': i, 'title': title, 'body': body, 'tokens': tokenify(doc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create keywords \n",
    "\n",
    "keywords = tokenify(keywords_data)\n",
    "keywords_vector = {}\n",
    "\n",
    "for keyword in keywords: \n",
    "    keywords_vector[keyword]= 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words\n",
    "bag_of_words = []\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    keywords_vec = keywords_vector.copy()\n",
    "    for token in doc['tokens']:\n",
    "        if token in keywords:\n",
    "            keywords_vec[token] += 1\n",
    "            \n",
    "    bag_of_words.append({'idx': i, 'vector': keywords_vec, 'max': max(keywords_vec.values())})\n",
    "    \n",
    "bag_of_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf representation\n",
    "from functools import reduce\n",
    "from math import sqrt\n",
    "tf = []\n",
    "\n",
    "for bag in bag_of_words:\n",
    "    idx = bag['idx']\n",
    "    vec = bag['vector']\n",
    "    max_n = bag['max']\n",
    "\n",
    "    keywords_vec = keywords_vector.copy()\n",
    "    \n",
    "    for key in vec.keys():\n",
    "        keywords_vec[key] = vec[key]/max_n\n",
    "    \n",
    "    vector_length = sqrt(sum_squared(keywords_vec.values()))\n",
    "    \n",
    "    tf.append({'idx': idx, 'vector': keywords_vec, 'vector_length': vector_length})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf representation\n",
    "from math import log10\n",
    "\n",
    "idf = keywords_vector.copy()\n",
    "\n",
    "document_count = len(documents)\n",
    "\n",
    "for key in idf.keys():\n",
    "    word_count = 0\n",
    "    \n",
    "    for doc in documents:\n",
    "        if key in doc['tokens']:\n",
    "            word_count += 1\n",
    "    \n",
    "    idf[key] = word_count / document_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf representation\n",
    "\n",
    "tfidf = []\n",
    "\n",
    "for i, doc in enumerate(tf):\n",
    "    keywords_vec = keywords_vector.copy()\n",
    "\n",
    "    for key in doc['vector'].keys():\n",
    "        keywords_vec[key] = doc['vector'][key] * idf[key]\n",
    "        \n",
    "    vector_length = sqrt(sum_squared(keywords_vec.values()))\n",
    "        \n",
    "    tfidf.append({'idx': i, 'vector': keywords_vec, 'vector_length': vector_length})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query \n",
    "query_string = 'Morgan Kaufmann Series in Machine Learning for Bioinformatics'\n",
    "\n",
    "query = tokenify(query_string)\n",
    "\n",
    "query_bag_of_words = {}\n",
    "query_bag_of_words['vector'] = keywords_vector.copy()\n",
    "\n",
    "for token in query_bag_of_words['vector']:\n",
    "    if token in query:\n",
    "        query_bag_of_words['vector'][token] += 1\n",
    "\n",
    "query_bag_of_words['max'] = max(query_bag_of_words['vector'].values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query tf\n",
    "vector = query_bag_of_words['vector']\n",
    "max_n = query_bag_of_words['max']\n",
    "\n",
    "query_tf = {}\n",
    "query_tf['vector'] = keywords_vector.copy()\n",
    "\n",
    "for key in vector.keys():\n",
    "    query_tf['vector'][key] = vector[key]/max_n\n",
    "\n",
    "query_tf['vector_length'] = sqrt(sum_squared(query_tf['vector'].values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query tfidf\n",
    "query_tfidf = {}\n",
    "query_tfidf['vector'] = keywords_vector.copy()\n",
    "\n",
    "for key in query_tf['vector'].keys():\n",
    "    query_tfidf['vector'][key] = query_tf['vector'][key] * idf[key]\n",
    "    \n",
    "query_tfidf['vector_length'] = sqrt(sum_squared(query_tfidf['vector'].values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure similarity!\n",
    "\n",
    "def cos_similiarity(a, b):\n",
    "    ab_sum = 0\n",
    "\n",
    "    for key in a['vector'].keys():\n",
    "        ab_sum += a['vector'][key]*b['vector'][key]\n",
    "    \n",
    "    return ab_sum / a['vector_length'] * b['vector_length']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate scores\n",
    "scores = []\n",
    "for doc_tfidf in tfidf:\n",
    "    scores.append({'idx': doc_tfidf['idx'], 'score': cos_similiarity(query_tfidf, doc_tfidf)})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2.0842203069370195 . Document title: Genetics-Based Machine Learning\n",
      "Score: 2.0393249717459705 . Document title: Journal of Machine Learning Research Homepage\n",
      "Score: 2.02627376418614 . Document title: The Machine Learning Systems Group at JPL | Home\n",
      "Score: 2.0058696685300634 . Document title: ICML-99\n",
      "Score: 1.9626210853613513 . Document title: Yahoo! Groups : machine-learning\n",
      "Score: 1.9584403753474926 . Document title: Machine Learning Research Software\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def show_best_matches(scores, docs, n = 10):\n",
    "    sorted_list = sorted(scores, key=itemgetter('score'), reverse=True)\n",
    "    \n",
    "    for i, elem in enumerate(sorted_list):\n",
    "        if i > 5:\n",
    "            return\n",
    "        \n",
    "        print('Score: {} . Document title: {}'.format(elem['score'], docs[elem['idx']]['title']))\n",
    "\n",
    "show_best_matches(scores, documents, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
